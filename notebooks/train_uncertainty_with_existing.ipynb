{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NN with uncertainty prediction (heteroskedastic regression)\n",
    "# from an existing model by training a network that takes the penultimate\n",
    "# hidden layer of the base network and predicting the standard deviation \n",
    "# of the error distribution.\n",
    "# Author: Peter Sadowski, Dec 2020\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os, sys\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' # Needed to avoid cudnn bug.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "sys.path = ['../'] + sys.path\n",
    "from sarhs.generator import SARGenerator\n",
    "from sarhs.heteroskedastic import Gaussian_NLL, Gaussian_MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model.\n",
    "file_model_src = './model.h5'\n",
    "model_base = load_model(file_model_src)\n",
    "\n",
    "# Define new output that predicts uncertainty. \n",
    "base_inputs = model_base.input\n",
    "base_penultimate = model_base.get_layer('dense_7').output\n",
    "base_output = model_base.output\n",
    "model_base.trainable = False\n",
    "\n",
    "x = Dense(256, activation='relu', name='std_hidden')(base_penultimate)\n",
    "std_output = Dense(1, activation='softplus', name='std_output')(x)\n",
    "\n",
    "output = concatenate([base_output, std_output], axis=-1)\n",
    "model = Model(inputs=base_inputs, outputs=output)\n",
    "\n",
    "import keras_extras\n",
    "importlib.reload(keras_extras.losses.dirichlet)\n",
    "from keras_extras.losses.dirichlet import Gaussian_NLL, Gaussian_MSE\n",
    "\n",
    "opt = Adam(lr=0.0001)\n",
    "model.compile(loss=Gaussian_NLL, optimizer=opt, metrics=[Gaussian_MSE])\n",
    "#new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3916/3916 [==============================] - 177s 45ms/step - loss: 0.6615 - Gaussian_MSE: 0.0895 - val_loss: 0.8105 - val_Gaussian_MSE: 0.1193\n",
      "Epoch 2/100\n",
      "3916/3916 [==============================] - 177s 45ms/step - loss: 0.6273 - Gaussian_MSE: 0.0893 - val_loss: 0.8119 - val_Gaussian_MSE: 0.1193\n",
      "Epoch 3/100\n",
      "3916/3916 [==============================] - 177s 45ms/step - loss: 0.6223 - Gaussian_MSE: 0.0892 - val_loss: 0.8064 - val_Gaussian_MSE: 0.1193\n",
      "Epoch 4/100\n",
      "2207/3916 [===============>..............] - ETA: 51s - loss: 0.6220 - Gaussian_MSE: 0.0893"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-4ece944382d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msarhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSARGenerator2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2017'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m history = new_model.fit(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/anaconda3/envs/t2/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/anaconda3/envs/t2/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/anaconda3/envs/t2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/anaconda3/envs/t2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/anaconda3/envs/t2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/anaconda3/envs/t2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/libs/anaconda3/envs/t2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/libs/anaconda3/envs/t2/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/anaconda3/envs/t2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filename = '/mnt/tmp/psadow/sar/sar_hs.h5'\n",
    "hp = {'bs':128}\n",
    "train = sarhs.generator.SARGenerator2(filename=filename, subgroups=['2015_2016', '2018'], batch_size=hp['bs'])\n",
    "valid = sarhs.generator.SARGenerator2(filename=filename, subgroups=['2017'], batch_size=hp['bs'])\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    epochs=5,\n",
    "    validation_data=valid,\n",
    "    #callbacks=clbks,\n",
    "    #steps_per_epoch=100\n",
    "    verbose= 1 if INTERACTIVE else 2,\n",
    "    )\n",
    "\n",
    "model.save('model_transfer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [14:35<00:00, 36.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Index(['hsNN', 'hsNN_std', 'timeSAR', 'latSAR', 'lonSAR'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with uncertainty estimates.\n",
    "from tqdm import tqdm \n",
    "\n",
    "def predict(model, dataset):\n",
    "    ys, yhats = [], []\n",
    "    for batch in dataset:\n",
    "        inputs, y = batch\n",
    "        yhat = model.predict_on_batch(inputs)\n",
    "        if y is not None:\n",
    "            y = y.reshape(-1,2)\n",
    "        else:\n",
    "            y = np.zeros((yhat.shape[0], 1))\n",
    "        ys.append(y)\n",
    "        yhats.append(yhat)\n",
    "    yhat = np.vstack(yhats)\n",
    "    y = np.vstack(ys)\n",
    "    return y, yhat\n",
    "\n",
    "def define_groups():\n",
    "    groups = {}\n",
    "    for isat in range(2):\n",
    "        for year in [2019]:\n",
    "            for imonth in range(12):\n",
    "                sat = 'A' if isat==1 else 'B'\n",
    "                month = imonth+1\n",
    "                name = f'S1{sat}_{year}{month:02d}S'\n",
    "                groups[name] = (isat, year, month)\n",
    "    return groups\n",
    "\n",
    "# Dataset\n",
    "filename = '/home/psadow/lts/preserve/stopa/sar_hs/data/alt/sar_hs_2019.h5' # Contains all processed 2019 data.\n",
    "for group, (isat, year, month) in tqdm(define_groups().items()):\n",
    "    # Make predictions for this group.\n",
    "    test = sarhs.generator.SARGenerator(filename, subgroups=[group], batch_size=200)\n",
    "    #print(test._num_examples())\n",
    "    _, yhat = predict(model,test)\n",
    "    \n",
    "    # The predictions should be in order.\n",
    "    # Include longitude, latitude, time, and file name.\n",
    "    df = pd.DataFrame()\n",
    "    df['hsNN'] = yhat[:,0]\n",
    "    df['hsNN_std'] = yhat[:,1]\n",
    "    df['timeSAR'] = test.h5file[group]['timeSAR'][:].flatten()\n",
    "    df['latSAR'] = test.h5file[group]['latlonSAR'][:, 0]\n",
    "    df['lonSAR'] = test.h5file[group]['latlonSAR'][:, 1]\n",
    "    Path(\"./predictions/\").mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(f'predictions/{group}.csv', index=False, )\n",
    "    \n",
    "print('Done')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune the prediction part on 2017 data (the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2073/2073 [==============================] - 177s 85ms/step - loss: 0.2593 - mae: 0.2593 - mse: 0.1528\n",
      "Epoch 2/5\n",
      "2073/2073 [==============================] - 157s 76ms/step - loss: 0.2560 - mae: 0.2560 - mse: 0.1487\n",
      "Epoch 3/5\n",
      "2073/2073 [==============================] - 157s 76ms/step - loss: 0.2537 - mae: 0.2537 - mse: 0.1457\n",
      "Epoch 4/5\n",
      "2073/2073 [==============================] - 157s 76ms/step - loss: 0.2518 - mae: 0.2518 - mse: 0.1443\n",
      "Epoch 5/5\n",
      "2073/2073 [==============================] - 157s 76ms/step - loss: 0.2500 - mae: 0.2500 - mse: 0.1420\n"
     ]
    }
   ],
   "source": [
    "# Load trained model.\n",
    "file_model = './models/model_45.h5'\n",
    "model_base = load_model(file_model)\n",
    "# Fine tune.\n",
    "opt = Adam(lr=0.00001)\n",
    "model_base.compile(loss='mae', optimizer=opt, metrics=['mae', 'mse'])\n",
    "history = model_base.fit(valid, epochs=5, verbose= 1 if INTERACTIVE else 2)\n",
    "# Loss initially at mse 0.15, more than 0.13 because of dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base.save('model_45_tuned.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Add back output that predicts uncertainty.\n",
    "base_inputs = model_base.input\n",
    "base_penultimate = model_base.get_layer('dense_7').output\n",
    "base_output = model_base.output\n",
    "model_std = load_model('./model_45_std.h5', custom_objects={'Gaussian_NLL':Gaussian_NLL, 'Gaussian_MSE': Gaussian_MSE})\n",
    "x = model_std.get_layer('std_hidden')(base_penultimate)\n",
    "std_output = model_std.get_layer('std_output')(x)\n",
    "output = concatenate([base_output, std_output], axis=-1)\n",
    "model = Model(inputs=base_inputs, outputs=output)\n",
    "# Compile and save.\n",
    "opt = Adam(lr=0.0001)\n",
    "from keras_extras.losses.dirichlet import Gaussian_NLL, Gaussian_MSE\n",
    "model.compile(loss=Gaussian_NLL, optimizer=opt, metrics=[Gaussian_MSE])\n",
    "model.save('model_45_std_tuned.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [09:18<00:00, 23.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Index(['hsNN', 'hsNN_std', 'timeSAR', 'latSAR', 'lonSAR'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with uncertainty estimates.\n",
    "from tqdm import tqdm \n",
    "\n",
    "def predict(model, dataset):\n",
    "    ys, yhats = [], []\n",
    "    for batch in dataset:\n",
    "        inputs, y = batch\n",
    "        yhat = model.predict_on_batch(inputs)\n",
    "        if y is not None:\n",
    "            y = y.reshape(-1,2)\n",
    "        else:\n",
    "            y = np.zeros((yhat.shape[0], 1))\n",
    "        ys.append(y)\n",
    "        yhats.append(yhat)\n",
    "    yhat = np.vstack(yhats)\n",
    "    y = np.vstack(ys)\n",
    "    return y, yhat\n",
    "\n",
    "def define_groups():\n",
    "    groups = {}\n",
    "    for isat in range(2):\n",
    "        for year in [2019]:\n",
    "            for imonth in range(12):\n",
    "                sat = 'A' if isat==1 else 'B'\n",
    "                month = imonth+1\n",
    "                name = f'S1{sat}_{year}{month:02d}S'\n",
    "                groups[name] = (isat, year, month)\n",
    "    return groups\n",
    "\n",
    "# Dataset\n",
    "filename = '/home/psadow/lts/preserve/stopa/sar_hs/data/alt/sar_hs_2019.h5' # Contains all processed 2019 data.\n",
    "for group, (isat, year, month) in tqdm(define_groups().items()):\n",
    "    # Make predictions for this group.\n",
    "    test = sarhs.generator.SARGenerator(filename, subgroups=[group], batch_size=200)\n",
    "    #print(test._num_examples())\n",
    "    _, yhat = predict(model,test)\n",
    "    \n",
    "    # The predictions should be in order.\n",
    "    # Include longitude, latitude, time, and file name.\n",
    "    df = pd.DataFrame()\n",
    "    df['hsNN'] = yhat[:,0]\n",
    "    df['hsNN_std'] = yhat[:,1]\n",
    "    df['timeSAR'] = test.h5file[group]['timeSAR'][:].flatten()\n",
    "    df['latSAR'] = test.h5file[group]['latlonSAR'][:, 0]\n",
    "    df['lonSAR'] = test.h5file[group]['latlonSAR'][:, 1]\n",
    "    Path(\"./predictions_std_tuned/\").mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(f'predictions_std_tuned/{group}.csv', index=False, )\n",
    "    \n",
    "print('Done')\n",
    "print(df.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
