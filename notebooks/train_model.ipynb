{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'sarhs.generator' from '/home/psadow/lts/preserve/stopa/sar_hs/sar_hs/sarhs/generator.py'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import os, datetime\n",
    "import socket\n",
    "from IPython import get_ipython\n",
    "HOSTNAME = socket.gethostname()\n",
    "INTERACTIVE = get_ipython() is not None\n",
    "if INTERACTIVE:\n",
    "    get_ipython().run_line_magic('env', 'CUDA_VISIBLE_DEVICES=0')\n",
    "SHERPA_TRIAL_ID = os.environ.get('SHERPA_TRIAL_ID', '0000')\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' # Needed to avoid cudnn bug.\n",
    "\n",
    "import sherpa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/psadow/lts/preserve/stopa/sar_hs/sar_hs/')\n",
    "import sarhs.generator \n",
    "import importlib\n",
    "importlib.reload(sarhs.generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model():\n",
    "    # Low-level features.\n",
    "    inputs = Input(shape=(72, 60, 2))\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = Conv2D(256, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    cnn = Model(inputs, x)\n",
    "\n",
    "    # High-level features.\n",
    "    inp = Input(shape=(32, ))  # 'hsSM', 'hsWW3v2', 'hsALT', 'altID', 'target' -> dropped\n",
    "    x = Dense(units=256, activation='relu')(inp)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    ann = Model(inputs=inp, outputs=x)\n",
    "    \n",
    "    # Combine\n",
    "    combinedInput = concatenate([cnn.output, ann.output])\n",
    "    x = Dense(256, activation=\"relu\")(combinedInput)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1, activation=\"softplus\")(x)\n",
    "    model = Model(inputs=[cnn.input, ann.input], outputs=x)\n",
    "    \n",
    "    opt = Adam(lr=0.00025897101528140915)\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def step_decay_schedule(initial_lr=1e-3, decay_factor=0.40, step_size=4):\n",
    "    '''Wrapper function to create a LearningRateScheduler with step decay schedule.'''\n",
    "    def schedule(epoch):\n",
    "        if epoch >= 10 and epoch < 20:\n",
    "            exponent = 1\n",
    "        elif epoch >= 20 and epoch <= 118:\n",
    "            exponent = 2\n",
    "        else:\n",
    "            exponent = 3\n",
    "        return initial_lr * (decay_factor ** exponent)\n",
    "    return LearningRateScheduler(schedule)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/123\n",
      "2469/2469 [==============================] - 59s 24ms/step - loss: 0.5048 - val_loss: 0.2062 - lr: 2.5897e-04\n",
      "Epoch 2/123\n",
      "2469/2469 [==============================] - 55s 22ms/step - loss: 0.3015 - val_loss: 0.2402 - lr: 2.5897e-04\n",
      "Epoch 3/123\n",
      "2469/2469 [==============================] - 64s 26ms/step - loss: 0.2589 - val_loss: 0.2012 - lr: 2.3307e-04\n",
      "Epoch 4/123\n",
      "2469/2469 [==============================] - 72s 29ms/step - loss: 0.2432 - val_loss: 0.1450 - lr: 2.3307e-04\n",
      "Epoch 5/123\n",
      "2469/2469 [==============================] - 74s 30ms/step - loss: 0.2288 - val_loss: 0.1592 - lr: 2.3307e-04\n",
      "Epoch 6/123\n",
      "2469/2469 [==============================] - 72s 29ms/step - loss: 0.2125 - val_loss: 0.1354 - lr: 2.0977e-04\n",
      "Epoch 7/123\n",
      "2469/2469 [==============================] - 59s 24ms/step - loss: 0.2048 - val_loss: 0.1479 - lr: 2.0977e-04\n",
      "Epoch 8/123\n",
      "2469/2469 [==============================] - 78s 32ms/step - loss: 0.1956 - val_loss: 0.1391 - lr: 1.8879e-04\n",
      "Epoch 9/123\n",
      "2469/2469 [==============================] - 79s 32ms/step - loss: 0.1847 - val_loss: 0.1348 - lr: 1.6991e-04\n",
      "Epoch 10/123\n",
      "2469/2469 [==============================] - 74s 30ms/step - loss: 0.1818 - val_loss: 0.1345 - lr: 1.6991e-04\n",
      "Epoch 11/123\n",
      "2469/2469 [==============================] - 73s 30ms/step - loss: 0.1762 - val_loss: 0.1325 - lr: 1.6991e-04\n",
      "Epoch 12/123\n",
      "2469/2469 [==============================] - 62s 25ms/step - loss: 0.1706 - val_loss: 0.1276 - lr: 1.6991e-04\n",
      "Epoch 13/123\n",
      "2469/2469 [==============================] - 58s 24ms/step - loss: 0.1670 - val_loss: 0.1267 - lr: 1.6991e-04\n",
      "Epoch 14/123\n",
      "2469/2469 [==============================] - 60s 24ms/step - loss: 0.1618 - val_loss: 0.1294 - lr: 1.6991e-04\n",
      "Epoch 15/123\n",
      "2469/2469 [==============================] - 56s 23ms/step - loss: 0.1562 - val_loss: 0.1320 - lr: 1.5292e-04\n",
      "Epoch 16/123\n",
      "2469/2469 [==============================] - 60s 24ms/step - loss: 0.1507 - val_loss: 0.1304 - lr: 1.3763e-04\n",
      "Epoch 17/123\n",
      "2469/2469 [==============================] - 60s 24ms/step - loss: 0.1456 - val_loss: 0.1284 - lr: 1.2387e-04\n",
      "Epoch 18/123\n",
      "2469/2469 [==============================] - 56s 23ms/step - loss: 0.1410 - val_loss: 0.1312 - lr: 1.1148e-04\n",
      "Epoch 19/123\n",
      "2469/2469 [==============================] - 60s 24ms/step - loss: 0.1363 - val_loss: 0.1259 - lr: 1.0033e-04\n",
      "Epoch 20/123\n",
      "2469/2469 [==============================] - 60s 24ms/step - loss: 0.1343 - val_loss: 0.1287 - lr: 1.0033e-04\n",
      "Epoch 21/123\n",
      "2469/2469 [==============================] - 56s 23ms/step - loss: 0.1297 - val_loss: 0.1259 - lr: 9.0298e-05\n",
      "Epoch 22/123\n",
      "2469/2469 [==============================] - 60s 24ms/step - loss: 0.1258 - val_loss: 0.1288 - lr: 8.1268e-05\n",
      "Epoch 23/123\n",
      " 288/2469 [==>...........................] - ETA: 35s - loss: 0.1200"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-bcfde9e5f0e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# train = datagen(gen_data, 101171//batch_size, file, 'train', batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# val = datagen(gen_data, 28906//batch_size, file, 'val', batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m history = model.fit(train,\n\u001b[0m\u001b[1;32m     26\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/anaconda3/envs/t2/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/anaconda3/envs/t2/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/anaconda3/envs/t2/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \"\"\"\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/anaconda3/envs/t2/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_process_logs\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;34m\"\"\"Turns tensors into numpy arrays or Python scalars.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/anaconda3/envs/t2/lib/python3.8/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/anaconda3/envs/t2/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/anaconda3/envs/t2/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/anaconda3/envs/t2/lib/python3.8/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    517\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/anaconda3/envs/t2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \"\"\"\n\u001b[1;32m    960\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/anaconda3/envs/t2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model = define_model()\n",
    "file_model = 'model.h5'\n",
    "#model.summary()\n",
    "#plot_model(model, to_file='model.png')\n",
    "\n",
    "# Dataset\n",
    "importlib.reload(sarhs.generator)\n",
    "batch_size = 128\n",
    "epochs = 123\n",
    "#filename = '/home/psadow/lts/preserve/stopa/sar_hs/data/alt/sar_hs.h5'\n",
    "filename = '/mnt/tmp/psadow/sar/sar_hs.h5'\n",
    "train = sarhs.generator.SARGenerator(filename=filename, split='2015_2016', batch_size=batch_size)\n",
    "valid = sarhs.generator.SARGenerator(filename=filename, split='2017', batch_size=batch_size)\n",
    "test = sarhs.generator.SARGenerator(filename=filename, split='2018', batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "#reduce_lr = step_decay_schedule(initial_lr=0.00025897101528140915, decay_factor=0.40, step_size=4)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=1) # This is slower than in paper.\n",
    "check = ModelCheckpoint(file_model, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "clbks = [reduce_lr, check, stop]\n",
    "\n",
    "# train = datagen(gen_data, 101171//batch_size, file, 'train', batch_size)\n",
    "# val = datagen(gen_data, 28906//batch_size, file, 'val', batch_size)\n",
    "history = model.fit(train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=valid,\n",
    "                    callbacks = clbks,\n",
    "                    verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/123\n",
      "2073/2073 [==============================] - 62s 30ms/step - loss: 0.1520 - val_loss: 0.1105 - lr: 7.3141e-05\n",
      "Epoch 2/123\n",
      "2073/2073 [==============================] - 47s 23ms/step - loss: 0.1443 - val_loss: 0.1233 - lr: 7.3141e-05\n",
      "Epoch 3/123\n",
      "2073/2073 [==============================] - 45s 22ms/step - loss: 0.1391 - val_loss: 0.1098 - lr: 6.5827e-05\n",
      "Epoch 4/123\n",
      "2073/2073 [==============================] - 45s 22ms/step - loss: 0.1363 - val_loss: 0.1096 - lr: 6.5827e-05\n",
      "Epoch 5/123\n",
      "2073/2073 [==============================] - 45s 21ms/step - loss: 0.1328 - val_loss: 0.1097 - lr: 6.5827e-05\n",
      "Epoch 6/123\n",
      "2073/2073 [==============================] - 46s 22ms/step - loss: 0.1292 - val_loss: 0.1094 - lr: 5.9244e-05\n",
      "Epoch 7/123\n",
      "2073/2073 [==============================] - 45s 22ms/step - loss: 0.1276 - val_loss: 0.1087 - lr: 5.9244e-05\n",
      "Epoch 8/123\n",
      "2073/2073 [==============================] - 45s 22ms/step - loss: 0.1252 - val_loss: 0.1122 - lr: 5.9244e-05\n",
      "Epoch 9/123\n",
      "2073/2073 [==============================] - 45s 22ms/step - loss: 0.1219 - val_loss: 0.1125 - lr: 5.3320e-05\n",
      "Epoch 10/123\n",
      "2073/2073 [==============================] - 45s 22ms/step - loss: 0.1192 - val_loss: 0.1155 - lr: 4.7988e-05\n",
      "Epoch 11/123\n",
      "2073/2073 [==============================] - 45s 22ms/step - loss: 0.1167 - val_loss: 0.1106 - lr: 4.3189e-05\n",
      "Epoch 12/123\n",
      "2073/2073 [==============================] - 45s 22ms/step - loss: 0.1153 - val_loss: 0.1123 - lr: 3.8870e-05\n",
      "Epoch 13/123\n",
      "2073/2073 [==============================] - 45s 22ms/step - loss: 0.1127 - val_loss: 0.1116 - lr: 3.4983e-05\n",
      "Epoch 14/123\n",
      "2073/2073 [==============================] - 45s 22ms/step - loss: 0.1113 - val_loss: 0.1131 - lr: 3.1485e-05\n",
      "Epoch 15/123\n",
      "2073/2073 [==============================] - 45s 22ms/step - loss: 0.1094 - val_loss: 0.1118 - lr: 2.8336e-05\n",
      "Epoch 16/123\n",
      "2073/2073 [==============================] - 45s 22ms/step - loss: 0.1079 - val_loss: 0.1131 - lr: 2.5503e-05\n",
      "Epoch 17/123\n",
      "2073/2073 [==============================] - 45s 22ms/step - loss: 0.1061 - val_loss: 0.1160 - lr: 2.2952e-05\n"
     ]
    }
   ],
   "source": [
    "test = sarhs.generator.SARGenerator(filename=filename, split='2018', batch_size=batch_size)\n",
    "\n",
    "history = model.fit(valid,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=test,\n",
    "                    callbacks = clbks,\n",
    "                    verbose = 1)\n",
    "model.save('model2_valid.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1447/1447 [==============================] - 32s 22ms/step - loss: 0.1288\n",
      "Epoch 2/10\n",
      "1447/1447 [==============================] - 23s 16ms/step - loss: 0.1231\n",
      "Epoch 3/10\n",
      "1447/1447 [==============================] - 23s 16ms/step - loss: 0.1209\n",
      "Epoch 4/10\n",
      "1447/1447 [==============================] - 23s 16ms/step - loss: 0.1183\n",
      "Epoch 5/10\n",
      "1447/1447 [==============================] - 23s 16ms/step - loss: 0.1167\n",
      "Epoch 6/10\n",
      "1447/1447 [==============================] - 24s 16ms/step - loss: 0.1154\n",
      "Epoch 7/10\n",
      "1447/1447 [==============================] - 24s 16ms/step - loss: 0.1136\n",
      "Epoch 8/10\n",
      "1447/1447 [==============================] - 24s 16ms/step - loss: 0.1121\n",
      "Epoch 9/10\n",
      "1447/1447 [==============================] - 24s 16ms/step - loss: 0.1109\n",
      "Epoch 10/10\n",
      "1447/1447 [==============================] - 24s 17ms/step - loss: 0.1099\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(test,\n",
    "                    epochs=10,\n",
    "                    verbose = 1)\n",
    "model.save('model2_test.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
