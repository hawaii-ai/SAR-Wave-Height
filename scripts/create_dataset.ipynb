{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'preprocess' from '/mnt/lts/nfs_fs02/sadow_lab/preserve/stopa/sar_hs/sar_hs/sarhs/preprocess/preprocess.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sarhs import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S: float32\n",
      "cspcIm: float32\n",
      "cspcRe: float32\n",
      "dt: float64\n",
      "dx: float64\n",
      "hsALT: float64\n",
      "hsSM: float64\n",
      "incidenceAngle: float64\n",
      "latALT: float32\n",
      "latSAR: float32\n",
      "lonALT: float32\n",
      "lonSAR: float32\n",
      "month: int64\n",
      "nk: float64\n",
      "normalizedVariance: float32\n",
      "satellite: int64\n",
      "sigma0: float32\n",
      "timeALT: float64\n",
      "timeSAR: float64\n",
      "year: int64\n",
      "Found 185199 events from years:  [2018]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Create new h5 file with following:\n",
    "# 1) Data separated by year for train/valid/test splits.\n",
    "# 2) Features scaled.\n",
    "groups = {'2015_2016':[2015, 2016], '2017':[2017], '2018':[2018]}\n",
    "file_src = '/home/psadow/lts/preserve/stopa/sar_hs/data/alt/aggregated_ALT.h5'\n",
    "file_dest = '/home/psadow/lts/preserve/stopa/sar_hs/data/alt/sar_hs.h5'\n",
    "\n",
    "# Print fields of source file.\n",
    "with h5py.File(file_src, 'r') as f:\n",
    "    for k in [k for k in f.keys()]:\n",
    "        print(f'{k}: {f[k].dtype}')\n",
    "\n",
    "# Create h5.\n",
    "with h5py.File(file_src, 'r') as fs, h5py.File(file_dest, 'w') as fd:\n",
    "    for group_name, years in groups.items():\n",
    "        grp = fd.create_group(group_name)\n",
    "        \n",
    "        # Find examples of these years.\n",
    "        indices = np.zeros_like(fs['year'][:], dtype='bool')\n",
    "        for year in years:\n",
    "            indices = np.logical_or(fs['year'][:] == year, indices)\n",
    "        num_examples = indices.sum()\n",
    "        print(f'Found {num_examples} events from years: ', years)\n",
    "        \n",
    "        # Write data from this year.\n",
    "        #print(fs['year'][indices].shape)\n",
    "        grp.create_dataset('year', data=fs['year'][indices])    \n",
    "        \n",
    "        # Get 22 CWAVE features. \n",
    "        cwave = np.hstack([fs['S'][indices,...], fs['sigma0'][indices].reshape(-1,1), fs['normalizedVariance'][indices].reshape(-1,1)])\n",
    "        cwave = preprocess.conv_cwave(cwave) # Remove extrema, then standardize with hardcoded mean,vars.\n",
    "        grp.create_dataset('cwave', data=cwave)\n",
    "        \n",
    "        # Additional features. \n",
    "        dx = preprocess.conv_dx(fs['dx'][indices])\n",
    "        dt = preprocess.conv_dt(fs['dt'][indices])\n",
    "        grp.create_dataset('dxdt', data=np.column_stack([dx, dt]))\n",
    "        \n",
    "        latSAR = fs['latSAR'][indices]\n",
    "        lonSAR = fs['lonSAR'][indices]\n",
    "        latSARcossin = preprocess.conv_position(latSAR) # Gets cos and sin\n",
    "        lonSARcossin = preprocess.conv_position(lonSAR)\n",
    "        grp.create_dataset('latlonSAR', data=np.column_stack([latSAR, lonSAR]))\n",
    "        grp.create_dataset('latlonSARcossin', data=np.hstack([latSARcossin, lonSARcossin]))\n",
    "        \n",
    "        timeSAR = fs['timeSAR'][indices]\n",
    "        todSAR = preprocess.conv_time(timeSAR)\n",
    "        grp.create_dataset('timeSAR', data=timeSAR, shape=(timeSAR.shape[0], 1))\n",
    "        grp.create_dataset('todSAR', data=todSAR, shape=(todSAR.shape[0], 1))\n",
    "        \n",
    "        incidence = preprocess.conv_incidence(fs['incidenceAngle'][indices]) # Separates into 2 var.\n",
    "        grp.create_dataset('incidence', data=incidence)\n",
    "        \n",
    "        satellite = fs['satellite'][indices]\n",
    "        grp.create_dataset('satellite', data=satellite, shape=(satellite.shape[0], 1))\n",
    "        \n",
    "        # Altimeter\n",
    "        hsALT = fs['hsALT'][indices]\n",
    "        grp.create_dataset('hsALT', data=hsALT, shape=(hsALT.shape[0], 1))\n",
    "        \n",
    "        # Get spectral data.\n",
    "        re = preprocess.conv_real(fs['cspcRe'][indices,...])\n",
    "        im = preprocess.conv_imaginary(fs['cspcIm'][indices,...])\n",
    "        x = np.stack((re, im), axis=3)\n",
    "        grp.create_dataset('spectrum', data=x)\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}